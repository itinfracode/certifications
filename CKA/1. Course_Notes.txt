Kubernetes :-

--> Core Components

    - List of Control plane and other components
    Master :- 
        - kubeapi
        - etcd
        - controller-manager
        - kube-scheduler
    Worker :-
        - kubelet
        - kube-proxy

    - API (kube-api) server that is the only component which talks to etcd server.

===================
 CORE CONCEPTS
===================

    --> Create NS

        # kubectl create ns dev-ns

    --> Create Service

        # kubectl create service clusterip redis-service --tcp=6379

POD & Service :-

    ---> Create the pod template file with dry-run

        $ kubectl run nginx --image=nginx --dry-run=client -o yaml > nginx.yaml

    --> Get pod by selecting multiple Labels

        # kubectl get pods --selector=env=prod,bu=finance,tier=frontend

    --> Create new POD / Expose it & Label it

        # kubectl run custom-nginx --image=nginx --port=8080 -l tier=db

    ---> Expose existing pod & create new service

        $ kubectl expose pod nginx --name=nginx-service --port=80

    --> Create Pod & Service (clusterIP) in a single command

        Create a new pod called & create a service of type ClusterIP with target port 80

        # kubectl run httpd --image=httpd:alpine --port=80 --expose

==============
Deployments :-
==============

    ---> Create a deployment without replica
        
        $ kubectl create deploy nginx-app --image=nginx

    --> Create deployment with replica

        # kubectl create deployment redis-deploy -n dev-ns --image=redis --replicas=2

    --> To set image on existing deployment

        # kubectl set image deployment <deployment name> <container name>=<image name>

    # Scale existing deployment

        $ kubectl scale deploy nginx-app --replicas=3

Label / Labeling :- 

--> Add Label - Some examples are

    - Add label on existing resource

        $ kubectl label <resource> <resource name> <label (key=value)>

        $ kubectl label node node01 color=blue

    # kubectl label pods my-pod new-label=awesome

    # kubectl label node node01 color=blue

    # Examples :- Get pods with labels

        $ ktl get pods --selector env=dev

    # Multiple selectors

        $ kgp --selector env=prod,bu=finance,tier=frontend

==============
Scheduling :-
==============

    - No pod will be scheduled without kube-scheduler pod in kube-system
    - Pod will be in "Pending" state

    1. DaemonSet
        - Almost same as ReplicaSet except kind is DaemonSet
        - Master node daemonsets
            - kube-proxy
            - Weavenet
            - flannel
                etc.

        - To see on how many nodes the DaemonSet is scheduled

            $ kubectl describe ds <ds name> <namespace> 

            And look for

            - Desired Number of Nodes Scheduled: 2
            - Current Number of Nodes Scheduled: 2

    2. ReplicationController / ReplicaSet

    3. Taints & Tolerations are meant to apply on nodes to accept certain pods to run on that node
        - It doesn't guarantee that tolerant pod will be deployed on tainted node, it just guarantee that no other in-tolerant pod gets deployed on that node.
        - Pod - Node guarantee achieved by "Node Affinity"

            --> Create a taint on node01 with key of 'spray', value of 'mortein' and effect of 'NoSchedule'
                Syntax
                    # kubectl taint nodes node1 key1=value1:NoSchedule
                    eg.
                    # kubectl taint node node01 spray=mortein:NoSchedule

            --> Add Tolerataion to Pod

                - https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts

            --> Remove taint from node

                Syntax
                # kubectl taint nodes <nodename> <taint>-
                    # taint value = Copy exact taint line from #kubectl describe node taint value
                    # add - (dash) at the end to remove
                Eg.
                # kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

    4. Scheduling

        - Add property "NodeName: <node01>" in Pod manifest spec section to specifically place that pod on that node.
    
    5. NodeSelector
        - Only single condition like ( NodeSelector = Large or NodeSelector= Medium)
        - Can't do like Place pod on NodeSelector Large or Medium or !small
        - To achieve that, we need NodeAffinity.
        - nodeSelector is a field of PodSpec (Spec)

    6. NodeAffinity
        - Add NodeAffinity in Pod manifest, so it can go any node which has NodeAffinity label added.
        - In Short, Assigning pods to nodes
        - Type of NodeAffinity
            - requiredDuringSchedulingIgnoredDuringExecution
            - preferredDuringSchedulingIgnoredDuringExecution
        - There two types of Affinity
            - nodeAffinity
                - To bind pod to node
    
    - Pod Affinity
        - Match label on pod and run on same node

    - Pod Anti-Affinity
        - Match label on pod and don't run same node

    - Taints & Tolerance vs NodeAffinity
        - Taint a node to restrict specific type of pods ( Tolerated ) to deploy there, but that Tolerated pod can go to "other" node, where it doesn't have taint.
        - So along with Taint&Tolerance, apply NodeAffinity to that pod so it doesn't go anywhere else.
        - And no "other" pod can be deployed on Tainted node.
        - If we do only Taint, then Tolerated pod can go to "Other" node, so in such case we need both (Taints & NodeAffinity)
        - In Simple words,
            - NodeAffinity - We are saying pod to go to specific node with Affinity rule.
            - Node Taints - We are restricting node, that no other pod (except tolerated pod), can come here.

    7. Resource Request / Limit
        - Request is like pod is requesting that amount of minimum resource from node before Scheduling.
        - Default Limit (For the POD to pick up those defaults you must have first set those default values for request and limit by creating a LimitRange in that namespace.)
            - CPU - 1 vCPU
            - Memory - 512 Mi
        - Limit is like pod should go up to that limit only
            - In case of CPU
                - K8S throttles cpu, so it can't go beyond that limit.
            - In case of Memory
                - K8S will terminate that pod if it goes beyond that memory limit.
        - Pod
            - CPU
                - 1 = 1 vCPU / 1 Core
                - We can specify like 0.1 / 100m ( m stands for mili )
                - Minimum - 1m
            - Memory
                - 256mi / 268435456 / 256M / 1G / 1Gi

            - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/
            - https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/
            - https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/

    8. Custom/Multiple Scheduler

        - While creating a pod, you can instruct in K8S to use specific scheduler while scheduling this pod.

        - Add "--scheduler-name=<custom scheduler name> in arguments

        - If multiple schedulers running on multiple master nodes, we have to specify --leader-elect=true as only one active scheduler must be there & --lock-object-name.

        - schedulerName is a field of PodSpec (Spec)
            - If the scheduler is not configured correctly, pod will be in a "Pending" state.

        - To deploy custom scheduler, copy existing schedular file from /etc/kubernetes/manifest & rename required fields and deploy.

                - Multi Scheduler Practice

                    - Add
                        --secure-port=0
                        --scheduler-name=my-scheduler
                    - Remove
                        - Startupprobe
                    - change
                        - name
                        --port=10282
                        --leader-elect=false
                            ( Leader-elect needed when you want to make cluster, here we are creating new schedular )
                        Replaced --bind-address to --address

        - If you are using existing file then change port number as well

        - Remove startupprobe porbe from existing file
    
    9. Static pods

        - Create manifest file inside kubelet local directory
            - on local node see using command
                # docker ps

            - To check kubelet directory
                1 - controlplane 
                    $ ps -aux |grep -i kubelet
                        And check for configured file in "--config"
                        usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml 
                        --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2

                2 - As per service configuration open file & look for "staticPodPath:"
                    # cat /var/lib/kubelet/config.yaml |grep staticPodPath
                    - Default path - /etc/kubernetes/manifests/

        - On Master node it can be viewed via
            # kubectl get pods

        --> Check static pod on local node
            # docker ps

        --> To see static pods
            # kubectl get pods -A
                - Look for the pods with name appended with master node
                    Eg. $ kube-scheduler-controlplane
                        - controlplane is master node here

==============
Monitoring :-
==============

    - Cluster / Node / Pod

    - # docker logs or kubectl logs

      # kubectl logs -f <pod name>

      # In case, multiple containers in a single pod run 
        # kubectl logs -f <pod name> <container name>

        --> To inspect metrics

            # kubectl top node/nodes
            # kubectl top pods

        --> To see logs of a pod/application

            # kubectl logs <pod name>
            # kubectl logs <pod name> <container name>

==============
App Update :-
==============

    - Two types (case sensitive)
        - Recreate (Delete all first and create new)
        - RollingUpdate (Delete one and create one)

        --> To see rollout status / history

            # kubectl rollout status deployment <deployment>

            # kubectl rollout status deployment <deployment>

============================
Commands & Arguments :-
============================

    - "command" overrides "entrypoint" (in docker)

    - "args" overrides "cmd" (in docker)

        --> Dry Run and create only yaml through kubectl command example (Note --dry-run=client and -o yaml)

        # kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 >> busybox.yaml

    --> in Yaml Try to add command in container like this (Try to avoid in single line)

        command:
        - sh
        - -c
        - sleep 20

============================
Yaml :- Refer Yaml lecture
============================

    1. key/value pair (Each separate item)
    2. Array/List (Name & Below that list  start with - [dash] at the front) (Only Values)
        - Ordered
        - If not same sequence, then it will be different
        - List title can be blank
        - Values can be added by - (dash)
            Eg.
                Fruits:
                - Apple
                - Mangos
                - Orange
                Vegetables:
                - Okra
                - Egg plants

                - These are two arrays.

    3. Dictionary (Set of properties grouped together / Grouping - start after 2 spaces) (Key: Value)
        - List of Dictionary
        - If not same sequence, then it will be different
        - Values can be added by key:value pair
            Eg.
                Fruits:
                    name: apple
                    color: red
                Vegetable:
                    name: Okra
                    color: green

        --> Now If you want to add nutritions in this, you must use "Dictionary in Dictionary"

                Fruit:
                    name: apple
                    color: red
                    nutritions:
                        calories: 100g
                        fat: 2g
                        iron: 4g
   
        --> Now If I want to add multiple fruits/vegetables in Dictionary, You must use "List of Dictionaries"
            Eg.
                    -   name: apple
                        color: red
                        type: fruit
                    -   name: mango
                        color: Yellow
                        type: fruit
                    -   name: Okra
                        Color: Green
                        type: vegetable
                    -   name: Egg Plant
                        Color: Violet
                        type: vegetable

        --> Now if you want to add multiple fruits and its nutritions, you have to use list of dictionaries in dictionary

                    - name: apple
                      color: red
                      nutritions:
                            calories: 100g
                            fat: 2g
                            iron: 4g
                    - name: mango
                      color: yellow
                      nutritions:
                            calories: 150g
                            fat: 20g
                            iron: 40g

        --> Array (With Title No key-value)

                    Vegetables:
                    - Egg Plant
                    - Okra

        --> List (No Title)

                - Egg Plant
                - Okra

        --> Array/List of a dictionary (with Title)

                    Vegetables:
                    -   name: Egg Plant
                        Color: Violet
                    -   name: Okra
                        Color: Green

    4. Any line begining with # is comment

==============
Configmap :-
==============

    - Imperative :-
        - kubectl create configmap
        - Eg.
            - #kubectl create configmap app-config --from-literal=APP_COLOR=blue
            (--from-literal it means key value pair in command itself )
            - #kubectl create configmap app-config --from-file=<filename>

        --> Create configmap

            # kubectl create configmap <configmap name> --from-literal=<key>=<value>

            # Example :-

                # kubectl create configmap colourconfigmap --fron-literal=APP_COLOR=blue

    - Declaritive :-
        - kubectl create -f <file>

==============
Secrets :-
==============

    - Convert normal data to hash (encode)
        # echo -n 'password/username' | base64
    - To decode
        # echo -n 'hashed password' | base64 --decode

    --> Create Generic/Custom Secret (without username/password as a key)

        # ktl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

    --> Decode Certificate 
        # openss x509 -in <certificate> -text -noout

    # Create a secret name my-secret with username shekhar

        $ kubectl create secret generic my-secret --from-literal username=shekharâ€‹

==============
OS Update :-
==============

    - If the node comes up immediately, kubelet starts and pods comes online.
    - If node is down for 5 mins, k8s consider that node to dead, If the pods were in Replicaset, then it will be recreated on another node.
    - If the pods were not a part of replicaset, it won't be created.

============================
Cluster Maintenance :-
============================

    - If the node is down for 5 mins, then kubernetes declare that nodea as dead and terminate pods on it 
        & if the pods are part of ReplicaSet, then they are re-created on another node.
        - The time it waits for pods to come online, is known as Pod Eviction timeout
            Can be set on controller manager like "#kube-controller-manager --pod-eviction-timeout=5m0s"
    - Maintenance 
        - #kubectl drain node-1 (Move all pods running on that node and make that node unschedulable)
            - Drain = Cordon + Unschedulable
        - #kubectl cordon node-1 (Just mark node unschedulable, it doesn't move pods to another node)
        - #kubectl uncordon node-1 (Aftre reboot, To make the node schedulable)

    ---> Cordon-Uncordon / Drain nodes
        
            - #kubectl drain node-1 (Move all pods running on that node and make that node unschedulable)
                - Drain = Cordon + Unschedulable
            - #kubectl cordon node-1 (Just mark node unschedulable, it doesn't move pods to another node)
            - #kubectl uncordon node-1 (Aftre reboot, To make the node schedulable)  

============================
Cluster Upgrade :-
============================

    - Kube-Api - None of other components should have higher version than kube-api (x)
        - Controller-manager (x-1)
        - Kube-Scheduler (x-1)
    - Kubelet / kube-proxy should have lower version than controller-manager & kube-scheduler (x-2)
    - Kubectl (utility) can be one version higher or lower than api
    - Kubernetes support recent 3 versions

    - First upgrade master & then upgrade worker nodes

        - If there are only 2 nodes (master/worker), upgrade only one at a time
            - Never use apt-get upgrade - always use update
            - If only 2 node cluster (one master/one worker) then check taint on master, to know it can accept pod or not.

                    - On Master

                        - Drain master node
                        - update kubeadm on master using apt
                        - kubeadm upgrade plan
                        - kubeadm upgrade apply <version>
                        - update kubelet
                        - systemctl restart kubelet/kubectl
                        - kubeadm uncordon masternode

                    - On slave

                        - Drain node from master
                        - update kubeadm using apt
                        - kubeadm upgrade node
                        - update kubelet/kubectl
                        - systemctl restart kubelet
                        - kubeadm uncordon node (from master)

                    - Verify status of nodes

                        - kubectl get nodes

============================
ETCD Backup / Restore :-
============================

    - etcd is deployed as a static pod in the cluster
    - To know the version of current etcd version, inspect image of the etcd pod
        # kubectl describe pod etcd-controller -n kube-system |grep -i image
    - ETCD stores information about states of the cluster
    - --data-dir is important, which needs to be backed up by backup tool.
    - To take snapshot
        - etcdctl snapshot save <snapshot name / snapshot.db>
    - To view status of the backup
        - etcdctl snapshot status <snapshot name>
        - Actual Command

            # etcdctl snapshot save <path of file> --endpoints --cacert --cert --key
                - You can check # etcdctl --help
                - endpoints, ca, cacert & key path can be obtained by # kubectl describe pod etcd -n kube-system

            # etcdctl snapshot save /opt/snapshot-pre-boot.db --endpoints=https://[127.0.0.1]:2379 --cert=/etc/kubernetes/pki/etcd/server.crt  \
                --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt

    - To Restore the backup
        # etcdctl restore snapshot <directory> <snapshot name> --endpoints --cacert --cert --key --initial-cluster-token=<new etcd token name>

        # edit etcd yaml in /etc/kubernetes/manifests
            - update VOLUME hostpath (not volumemount or command data-dir) to restored directory in manifest file.
            - Add --initial-cluster-token=<new etcd token name>

    - If you are using managed cluster, backup using quering cluster
        # kubectl get all --all-namespaces -o yaml >> all-backup.yaml

    - Always use "export ETCDCTL_API=3" before using etcd command
    - --listen-client-urls to know what ip is it listening.

    https://github.com/mmumshad/kubernetes-the-hard-way/blob/master/practice-questions-answers/cluster-maintenance/backup-etcd/etcd-backup-and-restore.md


====================================================================================
--> Authorization | Role & Role Bindings | Cluster Role & Cluster Role Bindings
====================================================================================

    - Node
        - Between Nodes - System:node certificate
    - ABAC ( Attribute Based )
        - Seperate policy for each user
    - RBAC ( Role Based )
        - Create a role and map that role to each user, so each time if you want to modify authorization, just update the role and mapping
        - Role is a set of permissions
    - Webhook
        - Outsource authorization mechanism, Install external agent where the access is granted

    - AlwaysAllow
        - Always allow without checking authorization rules
    - AlwaysDeny
        - Always deny without checking authorization rules

    - --authorization-mode=AlwaysAllow or any other mode in kube-api server

    - If we don't specify option (--authorization-mode) it is set to AlwaysAllow by deafult

    - We can spcify comma separated values like

        - --authorization-mode=Node,RBAC,Webhook
        - In this case, it handeled by Node authorizor, if it deny, then it is forwarded to next one
        - Once it is accepted, it doesn't go to next.

    - We can add "ResourceName" in manifest file, to provide more specific access in Role

        - Like access on Pod
        - Only Blue/Red Pod

    --> Authorization Flow

    - Process
        - Create Role object
        - Create role binding object
    - To view RBAC
        # kubectl get rbac
        # kubectl get rolebindings
    - To view role (developer)
        # kubectl describe role <Role Name>
        # kubectl describe role developer
    - To view rolebindigs
        # kubectl describe rolebindings <rolebinding name>
    - To check my access
        # kubectl auth can-i <permission>
            Eg. - # kubectl auth cani-i create deployments
                  # yes
                  # kubectl auth cani-i delete deployments
                  # no
    - To check access of a user as an admin
        # kubectl auth can-i create deployment --as <username>
            Eg. # kubectl auth can-i create deployment --as otheruser
                # yes
                # kubectl auth can-i delete deployment --as otheruser
                # no
        or
            # kubectl <command> --as <username>
            # kubectl get pods --as dev-user

        - We can specify different namespace in this command
            # kubectl auth can-i create deployment --as developer --namespace dev
            # yes
            # kubectl auth can-i create deployment --as developer --namespace prod
            # no
    - We can add "ResourceName" in manifest file, to provide more specific access
        - Like access on Pod
            - Only Blue/Red Pod

        - To check current authorization mode
            - Check kube-api pod configuration
                # kubectl describe pod kube-apiserver -n kube-system |grep -i authorization

        --> Example Commands to create role and role bindings
            - These role/rolebindings are namespaced ( They are created within namespace )
                - If we don't sppecify --namespace argument, it will be created in/for default namespace
            - ApiGroup is different for each resource like deployment/pod/volume etc, 
                so use specific ApiGroup for each resource them while creating role.

            - kubectl create role --help
            - kubectl create role developer --verb=create,get,delete --resource=pods --namespace=default
            - kubectl create rolebinding --help
            - kubectl create rolebinding dev-user-binding --role
            - kubectl edit role
            - kubectl edit rolebindings

    --> There are 2 types of resources
        - namespaced
        - Cluster
        - To list the namespaced & non-namespaced resource group
            # kubectl api-resources --namespaced=true
            or
            # kubectl api-resources --namespaced=false

    --> Clusterroles & Clusterrolebindings
        - They are same as role & rolebindings but they are for cluster resources (clusterwide)

        - If you create a cluster role and map it to a user, that user gets that access across the cluster in any namespace

        - Examples

            - To give all node access to user michelle
                - kubectl create clusterrole mnoderole  --verb=* --resource=node
                - kubectl create clusterrolebinding mnoderole --clusterrole=mnoderole --user=michelle

            - To give storage management access to michelle
                - kubectl create clusterrole storage-admin --verb=* --resource=persistentvolumes,storageclasses
                - kubectl create clusterrolebinding michelle-storage-admin --clusterrole=storage-admin --user=michelle

============================
--> User authentication
============================

    --> Certificate API | CSI

        - All the certificate related operations are carried out by Controller-Manager

        - To encode certificate and make it inline
            # cat certificate.csr |base64 -w0

        - To decode the certificate data
            # echo "encodedcontent" |base64 --decode
        
        - See the csr
            # kubectl get csr
            # kubectl certificate approve <name of certificate/csr>

            # To inspect the csr
                # kubectl describe <csr name>

            # To Deny csr
                # kubectl certificate deny <csr name>

            # To Delete CSR
                # kubectl delete csr <csr name>

==========================================
--> Securing Repository/Images
==========================================

    - image:nginx
        - It means image:nginx/nginx
                    - user or account name / image or repository
                    - If nothing specified, it uses same name for both
        - By default it goes to docker.io
                    - like image:docer.io/nginx/nginx
    - Pass credentials for private registry 
        1. kubectl create secret docker-registry regcred 
        2. Use those credentials in pod spec at imagePullSecrets

    --> Configure Credentials for private registry

        - kubectl create secret docker-registry regcred

            Eg. # kubectl create secret docker-registry private-reg-cred --docker-username=dock_user --docker-password=dock_password --docker-server=myprivateregistry.com:5000 --docker-email=dock_user@myprivateregistry.com

        - And use that secret in pod spec at "imagePullSecrets"

==========================================
--> SecurityContext / Capabilities
==========================================

    --> Security Context
        - Like runAsUser:
        - Capabilities 

        Note:
        - Security context can be added on pod and container both but Capabilities can be added to container only not on pod.
        - Check for another securityContext in yaml, if the one you added doesn't work.

    # root user id is "0", So if you want to run anything as root in container/pod use
        securityContext:
          runAsUser: 0

==========================================
--> Netowrk Policy (Firewall)
==========================================

    - Ingress (Incoming to resource)
    - Egress (Outgoing from resource)

    - Create Network policy to allow Ingress traffic from pod-name & Ports like Firewall
        - Create netowrk policy
        - Add policy type (Ingress/Egress)
        - PodSelector to apply policy to that pod

    - Network policies aren't supported on Flannel

    { } -- selects everything
    [ ] -- selects nothing

    - If you are blocking everything outgoing (egress), allow dns server access to resolve service name (both Udp/tcp ports)

            - ports:
                - protocol: TCP
                port: 53
                - protocol: UDP
                port: 53

    - Use separate from/to fields if you want to point separate pod on separate port

                                            apiVersion: networking.k8s.io/v1
                                            kind: NetworkPolicy
                                            metadata:
                                            name: internal-policy
                                            namespace: default
                                            spec:
                                            podSelector:
                                                matchLabels:
                                                name: internal
                                            policyTypes:
                                            - Egress
                                            egress:
                                            - to:
                                                - podSelector:
                                                    matchLabels:
                                                    name: payroll
                                                ports:
                                                - protocol: TCP
                                                port: 8080
                                            - to:
                                                - podSelector:
                                                    matchLabels:
                                                    name: mysql
                                                ports:
                                                - protocol: TCP
                                                port: 3306

                                            - ports:
                                                - port: 53
                                                  protocol: TCP
                                                - port: 53
                                                  protocol: UDP

    - To / From Any namesace
        - namespaceSelector: {}
    - To / From any Pod
        - podSelector: {}


==============
Volume :-
==============

    - Volume Mounting (New volume) & Bind Mounting (Existing Directory)

    - "Volumes" in "spec" & "VolumeMounts" in "containers"

        --> Volumes are pv to be shared

        --> VolumeMounts are to mount that volume into pod

    - PV (Persistent Volume) is a volume
    - PVClaim is to make that PV available on Node

    - PersistentVolumeReclaimPolicy : (Delete/Retain/Recycle)
        - Delete will delete PV when PersistentVolumeClaim is deleted
        - Retain will keep PV
        - Recycle will scrubbed before it will be available to another claim.

    - Volume Static Provisioning
        - You have to create volume on cloud provider and map it to PV manually

    - Volume Dynamic Provisioning
        - Volume will be created automatically when you create PV
        - You don't have to manually create PV, You just need to create storageClass & Point that storageClass into the PersistentVolumeClaim.
            - PV created automatically by storageClass

    - To mount host volume inside pod
            - Create Volume at spec level
            - Create volumemount inside container
            - Use "hostPath" in volume (When we want to use local directory in volume)

   - Check for accessmode if the PVClaim not showing available (May be size mismatch)

    - Inside Pod

        - Mount inside container as a volume
        - ReClaim Policy
            - Retain: When the PVC is deleted, the PV still exists. The volume is considered released, but it is not yet available because the previous data remains on the volume. If you want to delete it, you must do manually.
                - PV will not be deleted nor available

            - Delete: when the PVC is deleted, the PV and the associated storage in the external infrastructure (i.e. the Cinder storage in our case) are both deleted.
        
        - If PVC is being used in any pod, it can't be deleted.
        
        - PV status will show "Bound" if its bounded to PVC and it will show "Released" Once pvc is deleted.

    - If you use Storage Class, don't need to create PV, just create SC on cluster & PersistentVolumeClaim inside pod.

    - Understand available PV / PVC sizes
    
    - https://kubernetes.io/docs/concepts/storage/storage-classes/

    - kubectl get storageclasses/sc
        - Different Volume Binding modes
            - Immediate (default)
            - WaitForFirstConsumer

    - Notes while creating PVC
        - Access mode must match with PV
        - Storageclass

==============
initContainer
==============

    - Container inside container

    --> To check the state of the initContainer
        # kubectl describe pod 
            # check for "State" of that "initContainer"

====================================================================================
--> Kubeconfig ( Configuration file to save in home directory to switch context )
====================================================================================

    - Very Important

    - clusters
    - contexts
    - users

    - To view current config

        # kubectl config view
        # kubectl config view --kubeconfig=<file name>

    - To change context
        # kubectl config get-contexts
        # kubectl config use-context <context name>
        # kubectl config use-context dev-user@test-cluster-1 --kubeconfig=my-kube-config
        # If you want to use ~/.kube/config file ( don't give argument --kubeconfig)

            # kubectl config use-context <context name>

    - The permission on ~/.kube/config file must be "600" (rw - only by owner)

==============
Networking :-
==============

    - NodePort :- Port on node 
    - Port :- Port on Service
    - TargetPort :- Port on Pod (where pod is listening / application is deployed)

    - Network Namespace 
        - To check ip in current namespace
            # ip link
        - To check ip in a different namespace
            # ip netns exec <namespace> ip link / # ip -n red link

    - Docker Networking
        - Type of Networks
            - None
                - If we attach this network to anything, it can't connect to another network
                - To attach
                    # docker run --network none <container name>
            - Host Network
                - The container gets connected to host network
                - If the service inside container is running on port 80, it can be accessed on host ip without any configuration
                - If you try to run another container which runs on same port, it will not run as that port is already occupied
                - To attach
                    # docker run --network host <container name>
            - Bridge
                - An internal private network gets created and Container & Host attach to that network
                - We specify a subnet for this network
                - Each device / container connected to this network, gets a new ip in that range
                    # docker run --network bridge <container name>
        - To list Networks
            # docker network ls
        
    - CNI

        - Default cni contains 
            - Bridge / VLAN / IPVLAN / MACVLAN / WINDOWS - DHCP / Hostlocal
        - Plugins
            - Weave / Flannel / Cilium / NSX
        - Docker has CNM (Container network model)
        - docker0 is bridge interface

        - CNI Plugin is configured in kubelete on each node
            - networkplugin=cni
            - cni-bin directory has all the supported plugins as executables
            - cni-config directory has configuration
                - /etc/cni/net.d/

        - CNI plugin is responsible to assign ip address to containers
        
        - CNI Plugin (Weavenet)
            - Install its agent/service on each node
            - Weave create its own bridge on nodes with name "weave"
            - Weave installation 
                - kubectl apply -f <>
                - Weave peers are installed as daemonset
            - To check pod ip address range configured in plugin
                # ps -aux |grep -i weave & check for "--ipalloc-range"

    - Cluster Networking 
        - Port Configuration (should open)
            - Master
                - kube-api (6443)
                - kubelet (10250)
                - kube-scheduler (10251)
                - kube-controllermanager (10252)
                - etcd (2379)
                - Port 2380 is only for etcd peer-to-peer connectivity
            - Worker
                - kubelet (10250)
                - Services expose (30000-32767)
        - To check maximum connections on a port
            # netstat -anp |grep -i <service name>
                # netstat -anp |grep -i etcd
            
    - Service Networking
        - Service is virtual and clusterwide
        - It doesn't get created on any specific node
        - ClusterIP
        - NodePort - Exposed to all nodes in the cluster
        - Whenever a service is created, an iptables rule is created on all nodes for that ip and port forwarding requests to pod.
            - Like all requests coming to service ip 192.168.0.5:80 --> go to pod 10.0.1.5:80.
            - It is configured in kube-proxy that which mode to be used for proxying such request.
                - Available options are [ userspace | iptables | ipvs ]
                - If nothing specified in kube-proxy, it will default use iptables
                - kube-proxy --proxy-mode [ userspace | iptables | ipvs ] ...
        - Ip range of all services can be set/found in [--service-cluster-ip-range]
            # kubectl describe kube-apiserver & see for
                - --service-cluster-ip-range=<10.96.0.0/12>
        - Logs of kube-proxy shows the type of proxy configured, like
            - iptables
            - firewalld
        - Kube proxy log can be found at /var/log/kube-proxy.log
            - Which shows entry of iptables rule added.
    
    - CoreDNS

        - Whenever a service created, a dns-ip record created in kube-dns service in same Namespace
            - To access service in same namespace
                # curl http://web-server
            - To access service in another namespace ( "apps" namespace)
                # curl http://web-server.apps
                or it can be accessed via
                    # curl http://web-server.apps.svc.cluster.local

        - For each pod, kubernetes generates a name by replacing . (dot) with - (dash) and create dns record
            - Like 192.168.0.5 it creates record with name 192-168-0-5 & mapped with ip 192.168.0.5
                which can be accessed via
                # curl http://192-168-0-5.apps.pod.cluster.local
                    Where [apps] is namespace, if its in default namespace, then it will be like # curl http://192-168-0-5.default.pod.cluster.local
                    ( mind here that name for pods can be accessed with subdomain [pod] and services via [svc])

        - Configuration of coreDNS can be found at  
            - /etc/cordns/Corefile

        - Corefile is added to CoreDNS as a configmap object, to See
            - # kubectl get configmap -n kube-system

        - Whenever CoreDNS pod is deployed, it deploys "kube-dns" service, so ip address of "kube-dns" is configured as a nameserver in all pods, so
            - cat /etc/resolv.conf
              -> nameserver 10.0.5.55 (cluster ip of kube-dns service)

        - Kubelet is responsible to configure nameserver in /etc/resolv.conf file in all pods when they gets deployed

        - "host" command is alternative to "dig"

        - Only services can be queried via short name like
            # host web-server or web-server.default or web-server.default.svc or web-server.default.svc.cluster.local
                Except - web-server.default.svc.cluster (cluster at the end)
                where
                    default = namespace
            - To query pod we have to enter whole line like
                # host 10-244-5-55.default.pod.cluster.local
    - ingress
        - Service nodeport can allocate ports higher than 30,000.
        - Ingress controller (Service)
            - To host Multiple services on a single hosted domain
                - mywebsite.com/shopping
                - mywebsite.com/devices
                - mywebsite.com/subscription
        - Available Ingress controllers
            - Nginx | Traefik | Haproxy | Istio
        - Ingress Resources (configuration/rule)
        - Different Ingress controllers
            - GCP (https)/Load Balancer (GCE) - Google ones
            - Nginx
            - Contour
            - HAproxy
            - Traefik
            - Istio
        
        - Ingress Setup

            - Ingress Controller
                - Deployment
                    - Container
                    - Configmap
                    - Service (NodePort)
                    - Service Account
                        - roles
                        - Clusterroles
                        - Rolebindings

            - Ingress resources/rules
                - Single hostname and sub-directory based (url/directory)
                    - Ingress resource
                        - Backend - Service name & Service Port
                - Subdomain based (host)
                    - Ingress resource
                        - Backend - Service with Host field
                - If we don't specify host field, it consider it as * (All traffic coming to that rule)

    --> Ingress

        - To see deployed ingress controller
            - Check pods deployed in kube-system ns
            # kubectl get pods --all-namespaces/-n kube-system

        - Ingress Controller Deployment

            # ktl create ns ingress-space
            # kn ingress-space
            # ktl create configmap nginx-configuration
            # ktl create serviceaccount ingress-serviceaccount
            # Create role & Rolebindings
                - Role 
                    - Create/Get configmaps
                    - Get endpoints, namespaces, pods, secrets
                - RoleBinding
                    - Serviceaccount
                    - New role
            # Create ingress deployment
            # Create ingress service to expose above deployment
            # Create Ingress resources in application namespace
            
            # Ingress Resource (Rules) must be deployed in the namespace where actual services are deployed, not in ingress namespace

==========================================
Install Cluster using kubeadm :-
==========================================

    (You must add taint on Master node to prevent workload to be deployed on master)
    - Install Docker on all nodes
    - Install Kubeadm tool on all nodes
        - https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
    - Initialize Master
    - Setup Pod Network
    - Join Worker nodes

    - MultiMaster Setup
        - # kube-controller-manager --leader-elect true [other options]
            - To avoid multiple commands execution & run kube-controller in active-passive mode
        - Same for scheduler

    --> Install cluster using kubeadm

        - Installation steps :-

            - Disable Swap on all nodes
                $ vi /etc/fstab
                    - Comment swap line
                    - Reboot
                - sudo swapoff -a
            - check br_netfiler module loaded or not, if not, load module
                $ lsmod | grep br_netfilter
                $  sudo modprobe br_netfilter
            - Follow docker installation document
            - Install kubeadm
            - We don't need to configure group driver in docker
            - Follow "Using kubeadm to Create a Cluster" document
                - Initialize control plane node using pod network cidr & --apiserver-advertise-address arguments
            - Install Pod Network Addon
            - Join worker nodes
            - Run nginx and check

============================
--> Troubleshooting
============================

    - To Check logs
        # kubectl logs <pod name> -f

    - To login to container

        # ktl exec -it <pod name> -n <namespace> -- sh/bash

    - To check control plane components
        - To check log of previous pod (If cluster deployed by kubeadm)
            # kubectl logs <pod name> -f --previous

            - Verify all pods are running in kube-system namespace
            - Check logs
                # kubectl logs kube-apiserver

    - If control plane components are deployed as services (not by kubeadm)
        - Check control plane services
            # service kube-apiserver status
            # service kube-controller-manager status
            # service kube-scheduler status

        - To check logs
            # journalctl -u kube-apiserver etc.
            
            --> To view the kubelet log

            # journalctl -u kubelet -l

        --> Check docker 
            # docker ps & get container id
            # docker logs <container id> # to check logs of that container
            # docker logs <container id> -f # to check logs on the go

    - Cluster Troubleshooting
        - Cluster information can be checked
            # kubectl cluster-info
        - Check certificate path / mounted volume / volume names
        - Control plane manifest files should be there in /etc/kubernetes/manifests

    - Worker node Troubleshooting

        - Check kubelet & kube-proxy services on worker nodes

        - kubectl get nodes
        - kubectl describe node <nodename>
            - If node is out of disk - outofdisk flag it set to true
            - Same for other
            - When node is ready "Ready" flag is true, which is not an error.

        - When worker node stop communicating to master, above stats show "Unknown"
            - Check "LastHeartbeatTime" the time when node crashed.

        - Check worker node services kubelet/kube-proxy
            - cpu / memory / disk space on node
            - service kubelet status / systemctl status kubelet / 
                For additional log add "-l" like # systemctl status kubelet -l
            - journalctl -u kubelet

        - Check kubelet certificate
            - /var/lib/kubelet/worker.crt

        - Check kubelet process configuration file with

            # ps -ef |grep -i kubelet

        - Kubelet config file
            - When we check $ service kubelet status
                - We can see file path at "Drop-In"
                    - /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
            /var/lib/kubelet/config.yaml

        - Check kubelet config file & server port, which is api server port / Default - 6443

        - If you are updating config file, reload daemon and then restart service
            # systemctl daemon-reload
            # systemctl restart kubelet

        - Cluster information can be checked
            # kubectl cluster-info

        --> List all the events in current namespace

            # kubectl get events

    - Network Troubleshooting
    
        - Check if network addon is installed or not
        - Check controller component description / logs
        - Check configmaps

        - Kubeproxy mounts config from configmap, so we have to specify exact filename which inside configmap.
        - Check for volume and mounts for pod env & configurations

        - Check DNS resolution
            - If not work
                - Check kube-dns service in kube-system ns
                - Match its selector with core-dns pods

